{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis homework\n",
    "\n",
    "In this notebook, part of the exercise will be to use latex notation in jupyter notebooks in order to submit answers to homework questions. PLease make sure to submit your answer in the cell following the question cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA definition (Population case)\n",
    "If $x$ is a random vector with mean $\\mu$ and covariance matrix $\\Sigma$, then the principal component transformation is the transformation:\n",
    "$$ x \\rightarrow y = \\Gamma'(x-\\mu) $$\n",
    "where $\\Gamma$ is orthogonal, $\\Gamma'\\Sigma\\Gamma=\\Lambda$ is diagonal and $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p \\geq 0$. This representation of $\\Sigma$ follows from the spectral decomposition theorem. The $i^{th}$ principal component of $x$ may be defined as the $i^{th}$ element of vector $y$, namely as \n",
    "$$ y_i = \\gamma'_{(i)}(x-\\mu)$$\n",
    "Here $\\gamma'_{(i)}$ is the $i^{th}$ column of $\\Gamma$ and may be called the $i^{th}$ vector of principal component loadings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "Given the definition above, prove the following properties:\n",
    "\n",
    "(a) $E(y_i) = 0$\n",
    "\n",
    "(b) $Var(y_i) = \\lambda_i$\n",
    "\n",
    "(c) $Cov(y_i,y_j)=0, \\forall i \\ne j$\n",
    "\n",
    "(d) $Var(y_1) \\geq  Var(y_1) \\geq \\dots \\geq Var(y_p) \\geq 0$\n",
    "\n",
    "(e) $\\sum_{i=1}^{p}{Var(y_i)} = tr(\\Sigma)$\n",
    "\n",
    "(f) $\\prod_{i=1}^{p}{Var(y_i)} = |\\Sigma|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 1:\n",
    "\n",
    "(a) $E(y_i)= E( \\gamma'_{(i)}(x-\\mu)))= \\gamma'_{(i)}E((x-\\mu))=\\gamma'_{(i)}(E(x)-E(\\mu))= \\gamma'_{(i)}(\\mu-\\mu) = 0$\n",
    "\n",
    "(b) $Var(y_i) =  var( \\gamma'_{(i)}(x-\\mu)))=\\gamma'_{(i)}var(x-\\mu)\\gamma_{(i)}=\\gamma'_{(i)}\\Sigma\\gamma_{(i)}=\\lambda_i$ ,$\\mu=constant ,var(x-\\mu)=var(x)=\\Sigma$\n",
    "\n",
    "(c)$Cov(y_i,y_j)=Cov( \\gamma'_{(i)}(x-\\mu)), \\gamma'_{(i)}(x-\\mu)))=E((\\gamma'_{(i)}(x-\\mu))-E(\\gamma'_{(i)}(x-\\mu)))E((\\gamma'_{(j)}(x-\\mu))-E(\\gamma'_{(j)}(x-\\mu)))'=E((\\gamma'_{(i)}(x-\\mu))((\\gamma'_{(j)}(x-\\mu))'))-E((\\gamma'_{(i)}(x-\\mu))E((\\gamma'_{(j)}(x-\\mu))')=E((\\gamma'_{(i)}(x-\\mu))((\\gamma'_{(j)}(x-\\mu))'))=E(\\gamma'_{(i)}(x-\\mu)(x-\\mu)'\\gamma_{(j)})=\n",
    "\\gamma'_{(i)}E((x-\\mu)(x-\\mu)')\\gamma_{(j)}=\\gamma'_{(i)}\\Sigma \\gamma_{(j)}=\\gamma'_{(i)}\\lambda_j\\gamma_{(j)}=\\lambda_j \\gamma'_{(i)}\\gamma_{(j)}=\\lambda_j 0=0$  because $\\lambda_j$ and  $ \\gamma_{(j)}$ are eignvalue and eignvector of $\\Sigma$=covariace matrix of x and $\\gamma_{(i)},\\gamma_{(j)}$ are orthogonal eignvectors of $\\Sigma$.\n",
    "\n",
    "\n",
    "(d) $Var(y_i) = \\lambda_i $ we have : $\\lambda_1 \\geq \\lambda_2 \\geq \\lambda_3 \\geq... \\lambda_p\\geq 0 $ so $Var(y_1) \\geq  Var(y_1) \\geq \\dots \\geq Var(y_p) \\geq 0$\n",
    "\n",
    "(e) $\\sum_{i=1}^{p}{Var(y_i)} =\\sum_{i=1}^{p}{\\lambda_i}=tr(\\Sigma)$  \n",
    "\n",
    "because {$\\lambda_i$} are eignvalues of $\\Sigma$ because $\\Gamma$ is an orthogonal matrix contains the eignvectors of $\\Sigma$ and this decomposition $\\Gamma'\\Sigma\\Gamma=\\Lambda$  is unique which gives the result(The representation of $\\Sigma$ follows from the spectral decomposition theorem).\n",
    "\n",
    "(f)  $\\prod_{i=1}^{p}{Var(y_i)} =\\prod_{i=1}^{p}{\\lambda_i}= |\\Sigma|$ because($\\Lambda$ is diagonal elements eignvalues of $\\Sigma$ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Is the following statement true? If yes, prove it.\n",
    "\n",
    "No SLC (see slides for definition) of $x$ has variance larger than $\\lambda_1$ ( variance of first principal component )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 2:\n",
    "\n",
    "True\n",
    "\n",
    "$(SLC)=\\alpha' x$  \n",
    "\n",
    "$var((SLC))=var(\\alpha' x)=\\alpha'\\Sigma\\alpha$ ,with $\\Sigma$=the covariance matrix of x\n",
    " \n",
    " \n",
    "$max(var(SLC))=max(\\alpha'\\Sigma\\alpha)$ \n",
    "subject to $\\alpha'\\alpha  = 1$ \n",
    "\n",
    "To solve this optimization problem a Lagrange multiplier $k$ is introduced:\n",
    "$L(w, α) =\\alpha'\\Sigma\\alpha-k(\\alpha'\\alpha -1)$\n",
    "\n",
    "\n",
    "Differentiating with respect to $\\alpha$ ,\n",
    "$\\Sigma\\alpha=k\\alpha$ so $k$ is eignvalue and $\\alpha$  eignvalue of $\\Sigma$\n",
    "\n",
    "multiplying both sides by $\\alpha$:\n",
    "$\\alpha'\\Sigma\\alpha= k\\alpha'\\alpha=k$\n",
    "\n",
    "so we have to maximize the eignvalue to obtain the maximum variance:\n",
    "\n",
    "$k$ est an eignvalue of $\\Sigma$ and we have $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p \\geq 0$  so $max(var(SLC))=k= \\lambda_1$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Is the following statement true? If yes, prove it.\n",
    "\n",
    "If $\\alpha = a'x$ is a SLC of $x$ which is uncorrelated with the first $k$ principal components of $x$, then the variance of $\\alpha$ is maximized when $\\alpha$ is $(k+1)^{th}$ principal component of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 3:\n",
    "\n",
    "True\n",
    "$\\alpha=a'x$  \n",
    "\n",
    "\n",
    "$var(\\alpha)=var(a' x)=a'\\Sigma a$ ,with $\\Sigma$=the covariance matrix of x\n",
    " \n",
    " \n",
    "$max(var(\\alpha))=max(a'\\Sigma a)$ \n",
    "subject to $ a'a  = 1$ \n",
    "\n",
    "To solve this optimization problem a Lagrange multiplier $k$ is introduced:\n",
    "$L(w, α) =a'\\Sigma a-k(a'a-1)$\n",
    "\n",
    "\n",
    "Differentiating with respect to $a$ ,\n",
    "$\\Sigma a=k a $ so $k$ is eignvalue and $a$  eignvalue of $\\Sigma$\n",
    "\n",
    "multiplying both sides by $a$:\n",
    "$a'\\Sigma a= k a'a=k$\n",
    "\n",
    "so we have to maximize the eignvalue to obtain the maximum variance:\n",
    "\n",
    "$k$ est an eignvalue of $\\Sigma$ and we have $max(var(\\alpha))=k<\\lambda_k $ ,As we have $\\lambda_1 \\geq \\lambda_k \\geq\\lambda_{k+1} \\dots \\geq \\lambda_p \\geq 0$  so $max(var(\\alpha))=k=\\lambda_{k+1}$\n",
    "result:$\\alpha$ is the $(k+1)^{th}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Prove that the correlation coefficient $r_{ij}$ defined in slide 4/34 satifies the following property: $|r_{ij}| \\leq 1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question\n",
    "*according to the inequality of cauchy schwarz:  $|cov(x_i,x_j)|\\leq \\sqrt(var(x_i)var(x_j))$\n",
    "\n",
    "$\\sum_{i=1}^{n}{|(x_{rj}-x_j)(x_{ri}-\\bar x_i)|} \\leq \\sqrt(\\sum_{i=1}^{n}{(x_{rj}-\\bar x_j)^2) \\sqrt(\\sum_{i=1}^{n}(x_{ri}-x_i)^2})$\n",
    "\n",
    "So $|r_{ij}|=\\frac {\\sum_{i=1}^{n}{|(x_{rj}-\\bar x_j)(x_{ri}-\\bar x_i)|}} {\\sqrt(\\sum_{i=1}^{n}{(x_{rj}-\\bar x_j)^2) \\sqrt(\\sum_{i=1}^{n}(x_{ri}-\\bar x_i)^2}}\\leq 1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "What are the limitations of PCA?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 5:\n",
    "PCA is computationally expensive,also is not robust to outliers. PCA always considered the low variance components in the data as noise and recommend us to throw away those components. But, sometimes those components play a major role in supervised learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Explain briefly what K-means is and how it is or could be used in practice. What are the disadvantages?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 6:\n",
    "*K means is an unsupervised algorithm used to divide our data into similar groups (clusters)\n",
    "\n",
    "-First we have to place randomly K points  ,these points represent initial groups centroids (centers of K clusters).\n",
    "\n",
    "-Second, assign each point(data) to the group that has the closest centroid (euclidien distance).\n",
    "\n",
    "-Third, when all objects have been assigned we recalculate the new positions of the K centroids which are the mean of the cluster points.\n",
    "\n",
    "-Finaly, we repeat these steps untill convergents.\n",
    "\n",
    "=> We have to repeat the process for many values of K and choose the one which minimize the cost function J\n",
    "\n",
    "*Disadvantages:\n",
    "\n",
    "-Difficulty to find the correct number of clusters\n",
    "\n",
    "-may converge to local minimum and hard Assignment might lead to misgrouping\n",
    "\n",
    "-Lose information from heavy outliers\n",
    "\n",
    "-Does not work efficiently with complex geometrical shaped data(Mostly Non-Linear)\n",
    "\n",
    "-It's a randomized algorithm each time you run it you see different clusters and finding optimal cluster configuration is very challenging\n",
    "\n",
    "-the k-means model has no intrinsic measure of probability or uncertainty of cluster assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of gaussians question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Explain briefly what Mixture of Gaussians Model is and how it is or could be used in practice. What are the disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 7:\n",
    "\n",
    "It is model of probability distribution of a random variable(gaussian distribution). In many situation the distribution of the variable may not follow a single distribution so it is modeled as a mixture,it's a soft Expectation maximization this consists of two steps:\n",
    "\n",
    "\n",
    "-first step is The expectation step or E step: for each point,find weights encoding the probability of membership in each cluster\n",
    "\n",
    "-Second step is known as the maximization step or M step:which consists of maximizing the expectations calculated in the E step  with respect to the model parameters,for each cluster, update its location(emperical variance and expectation), normalization, and shape based on all data points, making use of the weights.\n",
    "\n",
    "-Repeat until converged\n",
    "\n",
    "*Disadvantages:\n",
    "\n",
    " -Difficult to interpret.\n",
    " \n",
    " -may converge to local minimum.\n",
    " \n",
    " -Initialization of clusters will be difficult when dimensionality of data is high.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
